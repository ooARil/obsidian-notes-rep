
1. Можно считать по чанкам по 1000 элементов - есть два варика:
	1. Пагинация:
		Но следующие проблемы:
			- Консиснтность данных не должна меняться с каждым запросом, а у нас данные будут обновляться каждые сутки.
			- Пагинация требует сортировки данных всей таблицы - это будет жестко
			- Пагинация потокобезопасна.

		Возможное решение проблемы:
			- Вроде как можно навесить уровень изоляции serializable, и тогда будет гуд.

	2. Курсор:
		Но следующие проблемы:
			- Консиснтность данных не должна меняться с каждым запросом, а у нас данные будут обновляться каждые сутки.
			- А уместен ли вообще на таком объеме данные курсор.
			- Курсор не потокобезопасный.

		Возможное решение проблемы:
			- Вроде как можно навесить уровень изоляции serializable, и тогда будет гуд.
			- Придется вырубить контекст

2. После чего обрабатываем данные чанком:
	1. Возможное дообогащение данные (хождения в НСИ, ЕСФЛ, РП)
	2. ФЛК
 
3. Записываем данные - в этом случае два варика:
	1. Это писать данные в JobExecution (тогда врайтер не нужен, а делаем просто слушатель на чанк):
		Но следующие проблемы:
			- Элементы все разные, у каждого свой регион и подразделение
			- Скорее всего будет забиваться оператива

		Возможное решение проблемы:
			- А принципе можно создавать какой-то уникальный ключ с постфиксом в виде номера, и туда складывать лист из обработанного чанка

	2. Писать данные в какую-то таблицу:
		Но следующие проблемы:
			- Элементы все разные, у каждого свой регион и подразделение
			- Если писать в каждую строку одно значение, то таблица может разрастись

		Возможное решение:
			- Можно чистить её каждый раз после окончания джобы (но хз, может ли мы чистить данные, или мы их просто помечаем на удаление)

4. Данные не прошедшие ФЛК нужно писать также в файл:
		Скорее всего их нужно также как-то группировать по регионам.



---

Исходя из всех этих проблем, будто напрашивается старая архитектура, но в ней есть тоже свои жирные минусы
- Из-за больших запросов, и обработки больших пачек данных - все становится медленно
- Плюс слишком сложная структура

---

Также необходимо предусмотреть механизм выгрузки всех невыгруженных







Использую Java, Spring

У меня есть задача, нужно выгружать данные с таблицы, после чего разбивать их на папки и файлы в зависимости от параметров.

Из-за того, что таблица большая, наверное, лучше это делать пакетной обработкой, используя Spring Batch.
Но я столкнулся с следующей проблемой, если я хочу обрабатывать чанки параллельно, то у меня не получится записывать в файл, т.к. придется их постоянно открывать и закрывать, что опасно.

Поэтому я хочу на одном шаге пакетной обработки, собрать все данные с таблицы, после чего по чанкам их обработать, а после чего в врайтере, записать в таблицу каждый обработанный ранее объект.
Записать нужно именно 

А после чего на следующем шаге использовать самописный тасклет, в котором взять все данные, и просто раскидать в нужные папки, и в нужные файлы


---

Короче че по итогу дейлика:
1. Создается временная таблица с идентифактором джобы, чанка, и поля json (или bson), в который записывается лист чанка.
2. После чего отдельным step-ом просто берем все это с бд, и начинаем разбивать на регионы, подразделения и делать файлы.

* Для плохих файлов также создается такая же таблица, и либо другим степом, либо тем же, также формриуем единный архив и кидаем его на ФТП.


---

Эту задачу будут запускать через Администрирование - т.е. нужно уже видеть эту задачу как реализацию сервиса BackgroundTask

Задача: Перевести выгрузку ЦИК на витрину ЦПОиБА:
1. Данные должны забираться с витрины
2. В выгрузке должен быть ФЛК
3. Данные прошедшие и не прошедшие ФЛК записываются по разному:
   Прошедшие ФЛК - формат:
	   - Представляют из себя архив с папками в виде кодов регионов, в которых лежат соответствующие подразделения в виде файлов формат excel и txt
	   - Данные файлы должны быть отправлены в ФХ и записаны в ??? таблицу (background_task - лучше не трогать!!!)

	Непрошедшие ФЛК - формат:
	- Представляют из себя архив с папками в виде кодов регионов, в которых лежат лежат соответствующие подразделения в виде файла формата txt (в этом файле в первых столбцах написан код ошибки ФЛК + id дела + подистема)
	- Представляет из себя также excel-файл (отчет) с описанием всех ошибок ФЛК по каждому региону
	- Должен быть отправлен на ФТП

План:
1. Реализовываю step-1 (Основной step-с с 3мя этапами: Получить данные, Обработать + ФЛК, Записать данные) - 7  дней (в случае затруднений на первом этапе - > 7 дней)
	1. Пишу reader и сущности, проверяю, что вьюшка в принципе позволяет использовать курсор или пагинацию: (в случае успеха - 1 день, в случае неудачи - ?)
	   - Использую предпочтительнее курсор, оцениваю скорость работы, и что он вообще работает.
	   - В случае неудачи: придется спрашивать у Леши Книгина и пытаться решать проблему
	2. Пишу processor, переношу все сервисы из РУ в проект Леши - 4 дня:
		Отождествление данных:
			1. Проверяю, какие данные отдает мне ридер, насколько они отождествлены
			2. Уточняю, нужно ли переносить костыльные отождествления, если что-то отсутствует в записи.
		Валидация данных:
			1. Переношу валидатор из РУ в проект.
			2. Прогоняю данные на тестах, убеждаюсь, что процент ФЛК адекватный и равен тому проценту, который выгружает обычная выгрузка.
	3. Реализовываю writer - 2 дня:
		1. Создаю таблицу для данных прошедших ФЛК: с id джобы, id чанка и json (который себя представляет лист объектов данных прошедших валидацию, которые будут записаны в файлы)
		2. Создаю таблицу для данных непрошедших ФЛК: с id джобы, id чанка и json (который себя представляет лист объектов данных прошедших валидацию, которые будут записаны в файлы)
		3. Записываю данные листами в нужные таблицы:
			* Данные не прошедшие ФЛК пишу в виде модели с полями, которые содержат инфу об ошибках
		4. Проверяю что нет конфликтов при многопотоке, и что данные обратно можно десериализовать.
		5. Проверяю вес колонок, в случае если слишком большой, то будут сереализовать в другой формат - ?.
	4. Тестирование + Исправление - 1-2 дня:
		1. Проверяю что все корректно работает в многопотоке, что данные не затираются

2. Реализовываю step-2 (step который формирует архив с данными прошедшими ФЛК: т.е. архив с регионами -> подразделениями -> кодами событий) - 2 дня (т.к. может возникнуть проблема с пространством имен файлов, что такие файлы уже созданы и лежат во временной хранилище, поэтому придется их сначала чистить - нужно проверять).
	1. Создаю tasklet, в котором получаю инфу по всем чанкам по идентификатору текущей джобы.
	2. После прохожусь по всем чанкам и группирую мапы (ключ - id-региона, значение - список подразделений)
	3. После при записи в файл подразделения, фильтрую все данные, и отсортировываю по коду события.
	4. После чего создаю папку с кодом региона, и в неё складываю все файлы по подразделению.
	5. После все оборачиваю в zip-архив
	6. ??? (Тут по идее нужно кидать файл в ФХ - но вот вопрос, а куда его нужно писать)
	7. Проверяю, что все корректно создается, нету конфликтов с пространством имен файлов.

3. Реализовываю step-3 (step который формирует архив с данными непрошедшими ФЛК: т.е. архив с регионами -> подразделениями -> кодами событий) - 2 дня (нужно убедиться, что ошибки ФЛК пишутся правильно)
	1. Создаю tasklet, в котором получаю инфу по всем чанкам по идентификатору текущей джобы.
	2. После прохожусь по всем чанкам и группирую мапы (ключ - id-региона, значение - список подразделений)
	3. После при записи в файл подразделения, фильтрую все данные, и отсортировываю по коду события.
	4. После чего формирую excel-файлы с статистикой по ошибкам ФЛК.
	5. ??? (Тут по идее нужно отправлять на ФТП - но вот вопрос, норм ли отправлять в тасклете)
	6. Проверяю, что все корректно создается, нету конфликтов с пространством имен файлов.


-----

1. На первом этапе - нельзя формировать файлы

1. Читаю ЦПОиБА
2. Создаем временную папку с регионом 01 (например)
3. Создаем временную папку с подразделением 01 (например)
4. Созд

---

Алгоритм

Нужно сделать сортировку по директориям регионов, директориям подразделений и директориям типов событий (но мне непонятно, что там будут за файлы, но я так понял, что ты сам знаешь)  
Этот шаг чтения из ЦПОиБА формирует дтошку, в которой хранится файловая структура. Дтошка пишется в контекст средствами gismu-background-task.  
Дтошка, которая отображает, в каких директориях лежат какие директории и файлы (я скину интерфейсы элемент> директория/файл).  
Запрос элементов по 1000 из НСИ - это полезное усложнение, которое значительно ускорит выгрузку, но это усложнение.  
Нужно будет формировать структуру, о которой я написал, и в эту структуру добавлять сгенеренный нами гуид, который также класть в соответствие с идентификаторами, которые нужно опросить во вне (НСИ, ЕСФЛ и т.д.)  
Я тебе пришлю ссылку на ветку, где мы с Кириллом Леонтьевым реализовали многопоточное чтение и много writer'ов по одному на каждый файл.

шаг №1. ты читаешь ЦПОиБА, формируешь файловую структуру и айдишники внешних систем для запроса (также тут ты кладёшь данные в файл для ФЛК)  
шаг №2. ты опрашиваешь внешние системы (и удаляешь дубликаты айдишников тут или на первом шаге) и формируешь файлы с мапой айдишник - информация  
шаг №3. ты формируешь кэш в оперативке из получившейся мапы  
шаг №4. ты обрабатываешь файлы, получившиеся в шаге №1, и формируешь файлы, которые будут обработаны для формирования отчета и файлы результирующие (csv на 167 полей)  
шаг №5. ты формируешь эксель, в который пишешь данные из файла, который сформирован на шаге №4  
шаг №6. формируем архив  
шаг №7. записываем в базу хорошие  
шаг №8. отправляем в кафку плохие  
шаг №7 и №8 можно выполнять параллельно

csv-заголовок - сформировать дто-струкутуру и положить в контект

Статистику нужно собирать на этапе №4 открыв синхронный writer, который будет инкрементировать показатели по категориям и писать в свой файл

[https://gitlab.akb-it.ru/GISMU2.0/blank-visa-inv/bp-registration-backend/-/commit/adfc24d40303323153d2d23d1437fa2c6559cd23](https://gitlab.akb-it.ru/GISMU2.0/blank-visa-inv/bp-registration-backend/-/commit/adfc24d40303323153d2d23d1437fa2c6559cd23)  
вот тут доработки и по партициям и по курсору и по параллельной записи в файлы


--- 

Более подробный алгоритм:

Выгрузка делается в виде пакетной обработки средствами Spring Batch.

Разделена следующие step-ы:
1. step1 - оформляем в виде reader/processor/writer: 
	reader: Считываем курсором записи с витрины ЦПОиБА по 1000.
	processor: Проводим ФЛК
	writter: Формируем файловую структуру:
	* Директория с делами непрошедшими ФЛК
	* Директория с делами прошедшими ФЛК
	* Отчет с ошибками с делами


---


В ветрину необходимо добавить следующие поля:

|   |   |   |
|---|---|---|
|**Наименование поля**|**Тип**|**Откуда достать**|
|Наименование региона создавшего дело|text|По regionId из НСИ (атрибут name)|
|Код региона создавшего дело|varchar(128)|По regionId из НСИ (атрибут code)|
|Наименование подразделения создавшего дело|text|По departmentId из НСИ (атрибут name)|
|Код подразделения создавшего дело|varchar(128)|По departmentId из НСИ (атрибут code)|
|Признак архивности подразделения|boolean<br><br>* если statusCode = ARCHIVED, то true<br><br>* иначе false|По departmentId из НСИ (атрибут statusCode)|
|Список синонимов по personId|не знаю, есть ли в dremio массивы  <br>* если да, то тип - array  <br>* если нет, то тип - text (значения просто перечислить через разделитель ',')|По personId из ЕСФЛ|