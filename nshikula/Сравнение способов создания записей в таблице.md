
# Цель

Проверить известные способы вставки / обновления данных в БД, собрать информацию о времени выполнения

# Предусловия

Тесты проводятся в рамках оптимизации процесса миграции, поэтому исходные данные будут опираться на характеристики миграции.

1. В миграции используется чанк из 10_000 записей, соответственно, максимальный размер единоразового залива данных = 10_000.
2. Один из тест-кейсов будет текущей реализацией залива миграции, чтобы была исходная точка для сравнения результатов.

# Проработка тест-кейсов

## Кейс №1: Insert Multiple Rows в несколько запросов

1. Текущий кейс миграции: чанк из 10_000 делится по 500 штук, insert происходит путём использования команды вида:

```sql
-- запрос №1
insert into table table1 (field1, field2)
VALUES 
	(...),
	(...),
	...
	(...);

-- запрос №2
insert into table table1 (field1, field2)
VALUES 
	(...),
	(...),
	...
	(...);
...
```

Особенность подобного кейса и необходимость лимитации в том, что подобный оператор поддерживает максимальную единовременную вставку только 1_000 строк. Чтобы вставить больше, нужно ещё раз вызывать insert.

## Кейс №2: Склейка операторов Insert.

Склейка операторов. Кейс заключается в том, чтобы за одну передачу запроса передать 10_000 операторов Insert, склеив строки с командами.
```sql
-- это всё 1 запрос
insert into table table1 (...) values (...); insert into table table1 () values (...); ...
```

## Кейс №3 (дополняющий): Промежуточная временная таблица

Кейс заключается в том, чтобы создать временную таблицу, наполнить её, а после перенести данные из неё в постоянную за 1 команду. Данный кейс невозможен самостоятельно и должен комбинироваться с иными кейсами. 

При этом, его быстродействие может сильно меняться в зависимости от кейса, который он дополняет. К примеру, есть вероятность, что с кейсом №1 это будет много быстрее, чем с кейсом №2, так как кейс №1 делает всё не за 1 команду и данные могут писаться на жёсткий диск базы, а кейс №2 может из-за выполнения всё сразу держать данные в памяти до окончания сессии и только потом записывать на диск, в этом случае временная таблица прироста не даст.

Данный кейс нужно выполнить в комбинации с кейсом №1 и кейсом №2, таким образом, получим больше информации о предполагаемом поведении postgres в случае нескольких запросов в рамках 1 сессии и одного запроса целиком.

Таким образом, мы, данный кейс делится на следующие:
- Кейс № 3_1 - временная таблица в комбинации с кейсом №1.
- Кейс № 3_2 - временная таблица в комбинации с кейсом №2.

Наброски таких скриптов: 
```sql
-- Если в рамках 1 чанка запросов несколько - команду следует вызывать заранее в виде отдельного запроса, если же чанк планируется передавать полность - команду можно склеить со всеми последующими
create temporary table temp_table (...);

-- Если в рамках 1 чанка запросов несколько - insert будет вызываться несколько раз, в противном случае - только 1 раз. Комада ниже просто шаблон, реализация склейки зависит от кейса.
insert into temp_table (...) values (...);

-- Вызывается 1 раз после того, как все данные чанка были перемещы во временную таблицу. Это своего рода flush перед завершением транзакции, так как потом временная таблица автоматически удалится.
insert into target_table (...)
select (...)
from temp_table;
```

## Кейс №4 (комбинация кейсов) - Склейка операторов Insert Multiple Rows, чтобы отправить их все за 1 вызов

Смесь кейса №2 и кейса №1.
То есть отправляется всё за 1 запрос, запрос будет иметь вид:
```sql
insert into table table1 (field1, field2)
VALUES 
	(...),
	(...),
	...
	(...);
insert into table table1 (field1, field2)
VALUES 
	(...),
	(...),
	...
	(...);
```

От кейса №1 здесь идея использования разновидности insert под названием insert multiple rows. Ходят слухи, что он работает быстрее аналогичного числа обычных insert (ну и информации по сети мы передаём меньше).
От кейса №2 здесь идея склеить операторы insert и передать за 1 раз все.

## Кейс №5 - Использование библиотеки https://github.com/PgBulkInsert/PgBulkInsert

Если верить описанию, библиотека представляет из себя обёртку над командой COPY. Много где пишется, что copy работает быстрее Insert, однако эксплуатирование данной команды задача нетривиальная, так как работает она только на чтение из файла в таблицу. С этим и помогает библиотека.

Немного о принципе работы библиотеки. Если почитать документацию постгреса, можно увидеть в примерах, что не обязательно использовать файл в качестве источника, а можно читать из stdin. https://www.postgresql.org/docs/current/sql-copy.html

Вот, кстати, пример, как происходит чтение из stdin: https://copyprogramming.com/howto/usage-of-copy-from-stdin-in-postgres. Не сказать, что это прям полезно, но то, что так можно, это важный нюанс.

Сам принцип работы крутится вокруг вот такой команды, которую можно найти в исходниках либы: 

![[image_2024-02-06_21-02-07.png]]

И как можно подумать, наши данные передаются как stdin в бинарном виде по сети (я так думаю). Для этого постгрес предоставляют специальное API (низкоуровневое, так что ничего хорошего) в своём драйвере и вот такой класс, который отвечает за доставку контента в базу:

![[image_2024-02-06_21-03-23.png]]

Теперь по минимуму понятно, как это хотя бы работает. Мы имеем обёртку для трансформации данных + предоставленное API для быстрой доставки специально для команды COPY (которую нужно инициировать самому). 

Наверное, это даже как-то можно сделать красиво и высокоуровнево и назвать `MySuperFastBulkInsert9000NewGenerationService`.

## Кейс №6 - Использование JdbcTemplate .batchUpdate

JdbcTemplate предоставляет возможность сделать обновление данных пачкой. Для любого изменений данных используется .batchUpdate. В случае insert передается команда insert в sql строке.
Вот гайд: https://www.baeldung.com/spring-jdbc-batch-inserts.
И вот гайд: https://docs.spring.io/spring-framework/reference/data-access/jdbc/advanced.html.

В целом, функционал именно исполнения батча предоставляет постгресовый драйвер, поэтому при желании можно и ещё ниже опуститься,, до чистого jdbc.

## Кейс №7 - Использование одной транзакции в параллельном режиме

Данный кейс представляет собой модификацию кейса №1 с попыткой вызова операций insert параллельно из разных потоков в рамках единой транзакции.

# Исходные предположения

- Есть предположение, что insert multiple rows имеет оптимизации на стороне postgres и будет выполняться быстрее, чем аналогичное количество просто операторов insert.

- Есть предположение, что основной эффект временной таблицы достигается за счёт того, что у нас ну явно не топовые ssd стоят на сервер с БД и получает замедления записи из-за переноса на диск.

- Отправлять кучу склеенных запросов может быть выгодно за счёт экономии на сети.

- Возможно, jdbcTemplate batch operations не склеивают строки, а как-то иначе передают несколько запросов, в этом случае возможны распараллеливания и оптимизации со стороны postgres.

- Оператор copy быстрее, чем insert, операции, поэтому от bulk insert библиотеки ожидается наибольшая скорость вставки.

Предположения о результатах до начала эксперимента:
1. Самым медленным будет запись в несколько запросов
2. Запись нескольких запросов в промежуточную таблицу будет быстрее обычной.
3. Ещё быстрее будет склеивание insert.
4. Склеивание insert с промежуточной записью во временную таблицу в лучшем случае не даст прироста в скорости, но ожидаемо скорость просядет немного.
5. Запись склеенных multiple rows insert будет быстрее обычной склейки.
6. jdbcTemplate.batchUpdate покажет себя в худшем случае на уровне склеенных multiple rows insert, в лучшем - окажется быстрее.
7. Самым быстрым будет bulk insert через copy.
8. По параллельной записи предположений нет.
# Подготовка к проведению тестирования

Итак, нужно подготовить среду:
- Все тесты не должны аффектить другие, для этого каждый будет работать со своей таблицей.
- Общий объём информации для вставки: 500_000 записей.
- Изначальный объем каждой таблицы 100_000 одинаковых записей записей.
- Все изменения должны быть обёрнуты в транзакцию, чтобы откатить их. И не портить данные для повторных тестов.
- Было бы неплохо в конце сформировать excel файл или типа того, но просто логи тоже сойдут.
- Для тестирования выберем таблицу core_person_document, там много полей, индексы разные, это покажет разного рода задержки. Нужно только внешние ключи удалить, если они там есть.

Скрипт создания таблицы для теста лежит в папке ./data

## Проведение эксперимента

Реализация находится в ветке рег. учёта. Ссылки:
- На ветку: 
- На входную точку бенчарка:

## Анализ результатов



